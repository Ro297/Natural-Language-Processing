{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./india-news-headlines.csv\")\n",
    "data=data[['publish_date','headline_text']].drop_duplicates()\n",
    "data['publish_date']=pd.to_datetime(data['publish_date'],format=\"%Y%M%d\")\n",
    "data['year']=data['publish_date'].dt.year\n",
    "nlp= spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code takes a really long time, so there's pickled versions of those files\n",
    "\n",
    "### Get imp words by year\n",
    "import sklearn.feature_extraction.text as text\n",
    "def get_imp(bow,mf,ngram):\n",
    "    tfidf=text.CountVectorizer(bow,ngram_range=(ngram,ngram),max_features=mf,stop_words='english')\n",
    "    matrix=tfidf.fit_transform(bow)\n",
    "    return pd.Series(np.array(matrix.sum(axis=0))[0],index=tfidf.get_feature_names()).sort_values(ascending=False).head(100)\n",
    "\n",
    "\n",
    "### Global trends\n",
    "bow=data['headline_text'].tolist()\n",
    "total_data=get_imp(bow,mf=5000,ngram=1)\n",
    "total_data_bigram=get_imp(bow=bow,mf=5000,ngram=2)\n",
    "total_data_trigram=get_imp(bow=bow,mf=5000,ngram=3)\n",
    "\n",
    "\n",
    "### Yearly trends\n",
    "imp_terms_unigram={}\n",
    "for y in data['year'].unique():\n",
    "    bow=data[data['year']==y]['headline_text'].tolist()\n",
    "    imp_terms_unigram[y]=get_imp(bow,mf=5000,ngram=1)\n",
    "imp_terms_bigram={}\n",
    "for y in data['year'].unique():\n",
    "    bow=data[data['year']==y]['headline_text'].tolist()\n",
    "    imp_terms_bigram[y]=get_imp(bow,mf=5000,ngram=2)\n",
    "imp_terms_trigram={}\n",
    "for y in data['year'].unique():\n",
    "    bow=data[data['year']==y]['headline_text'].tolist()\n",
    "    imp_terms_trigram[y]=get_imp(bow,mf=5000,ngram=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = './Pickle files/total_data.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(total_data,outfile)\n",
    "outfile.close()\n",
    "\n",
    "filename = './Pickle files/total_data_bigram.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(total_data_bigram,outfile)\n",
    "outfile.close()\n",
    "\n",
    "filename = './Pickle files/total_data_trigram.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(total_data_trigram,outfile)\n",
    "outfile.close()\n",
    "\n",
    "filename = './Pickle files/imp_terms_unigram.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(imp_terms_unigram,outfile)\n",
    "outfile.close()\n",
    "\n",
    "filename = './Pickle files/imp_terms_bigram.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(imp_terms_bigram,outfile)\n",
    "outfile.close()\n",
    "\n",
    "filename = './Pickle files/imp_terms_trigram.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(imp_terms_trigram,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=pd.read_pickle('./Pickle files/total_data.pkl')\n",
    "total_data_bigram=pd.read_pickle(\"./Pickle files/total_data_bigram.pkl\")\n",
    "total_data_trigram=pd.read_pickle(\"./Pickle files/total_data_trigram.pkl\")\n",
    "\n",
    "f=open(\"./Pickle files/imp_terms_unigram.pkl\",\"rb\")\n",
    "d=f.read()\n",
    "imp_terms_unigram=pickle.loads(d)\n",
    "f.close()\n",
    "f=open(\"./Pickle files/imp_terms_bigram.pkl\",\"rb\")\n",
    "d=f.read()\n",
    "imp_terms_bigram=pickle.loads(d)\n",
    "f.close()\n",
    "f=open(\"./Pickle files/imp_terms_trigram.pkl\",\"rb\")\n",
    "d=f.read()\n",
    "imp_terms_trigram=pickle.loads(d)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Common unigrams across all the years\n",
    "common_unigram={}\n",
    "for y in np.arange(2001,2017,1):\n",
    "    if y==2001:       \n",
    "        common_unigram[y]=set(imp_terms_unigram[y].index).intersection(set(imp_terms_unigram[y+1].index))\n",
    "    else:\n",
    "        common_unigram[y]=common_unigram[y-1].intersection(set(imp_terms_unigram[y+1].index))\n",
    "\n",
    "        \n",
    "\n",
    "### Common bigrams across all the years\n",
    "common_bigram={}\n",
    "for y in np.arange(2001,2017,1):\n",
    "    if y==2001:\n",
    "         common_bigram[y]=set(imp_terms_bigram[y].index).intersection(set(imp_terms_bigram[y+1].index))\n",
    "    else:\n",
    "        common_bigram[y]=common_bigram[y-1].intersection(set(imp_terms_bigram[y+1].index))\n",
    "\n",
    "\n",
    "### Common trigrams, 1 year window\n",
    "common_trigram_1yr={}\n",
    "for y in np.arange(2001,2017,1):\n",
    "    common_trigram_1yr[str(y)+\"-\"+str(y+1)]=set(imp_terms_trigram[y].index).intersection(set(imp_terms_trigram[y+1].index))\n",
    "\n",
    "    \n",
    "### Commin trigrams, 2 year window\n",
    "common_trigram_2yr={}\n",
    "for y in np.arange(2001,2015,3):\n",
    "    if y==2001:\n",
    "        common_trigram_2yr[str(y)+\"-\"+str(y+1)+\"-\"+str(y+2)]=set(imp_terms_trigram[y].index).intersection(set(imp_terms_trigram[y+1].index)).intersection(set(imp_terms_trigram[y+2].index))\n",
    "    else:\n",
    "        common_trigram_2yr[str(y)+\"-\"+str(y+1)+\"-\"+str(y+2)]=set(imp_terms_trigram[y].index).intersection(set(imp_terms_trigram[y+1].index)).intersection(set(imp_terms_trigram[y+2].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "total_data.head(20).plot(kind=\"bar\",figsize=(25,10),colormap='Set2')\n",
    "plt.title(\"Unigrams\",fontsize=30)\n",
    "plt.yticks([])\n",
    "plt.xticks(size=20)\n",
    "plt.subplot(1,3,2)\n",
    "total_data_bigram.head(20).plot(kind=\"bar\",figsize=(25,10),colormap='Set2')\n",
    "plt.title(\"Bigrams\",fontsize=30)\n",
    "plt.yticks([])\n",
    "plt.xticks(size=20)\n",
    "plt.subplot(1,3,3)\n",
    "total_data_trigram.head(20).plot(kind=\"bar\",figsize=(25,10),colormap='Set2')\n",
    "plt.title(\"Trigrams\",fontsize=30)\n",
    "plt.yticks([])\n",
    "plt.xticks(size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,18,1):\n",
    "    plt.subplot(9,2,i)\n",
    "    imp_terms_bigram[2000+i].head(5).plot(kind=\"barh\",figsize=(20,25),colormap='Set2')\n",
    "    plt.title(2000+i,fontsize=20)\n",
    "    plt.xticks([])\n",
    "    plt.yticks(size=20,rotation=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,18,1):\n",
    "    plt.subplot(9,2,i)\n",
    "    imp_terms_trigram[2000+i].head(5).plot(kind=\"barh\",figsize=(20,25),colormap=\"Set2\")\n",
    "    plt.title(2000+i,fontsize=20)\n",
    "    plt.xticks([])\n",
    "    plt.yticks(size=15,rotation=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count of common tokens across the years\n",
    "count_common_bi={}\n",
    "for year in range(2001,2017,1):\n",
    "    count_common_bi[year]=pd.Series()\n",
    "    for word in common_bigram[year]:\n",
    "        if year==2001:\n",
    "            count_common_bi[year][word]=imp_terms_bigram[year][word]+imp_terms_bigram[year+1][word]\n",
    "        else:\n",
    "            count_common_bi[year][word]=count_common_bi[year-1][word]+imp_terms_bigram[year+1][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,17,1):\n",
    "    plt.subplot(9,2,i)\n",
    "    count_common_bi[2000+i].sort_values(ascending=False).head(10).plot(kind=\"barh\",figsize=(20,35),colormap=\"Set2\")\n",
    "    if (2000+i)==2001:\n",
    "        plt.title(str(2000+i)+\"-\"+str(2000+i+1),fontsize=30)\n",
    "    else:\n",
    "        plt.title(\"upto-\"+str(2000+i+1),fontsize=30)\n",
    "    plt.xticks([])\n",
    "    plt.yticks(size=20,rotation=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Story of 'year old'\n",
    "index=data['headline_text'].str.match(r'(?=.*\\byear\\b)(?=.*\\bold\\b).*$')\n",
    "texts=data['headline_text'].loc[index].tolist()\n",
    "noun=[]\n",
    "verb=[]\n",
    "for doc in nlp.pipe(texts,n_threads=16,batch_size=10000):\n",
    "    try:\n",
    "        for c in doc:\n",
    "            if c.pos_==\"NOUN\":\n",
    "                noun.append(c.text)\n",
    "            elif c.pos_==\"VERB\":\n",
    "                verb.append(c.text)            \n",
    "    except:\n",
    "        noun.append(\"\")\n",
    "        verb.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "pd.Series(noun).value_counts().head(10).plot(kind=\"bar\",figsize=(20,5),colormap=\"Set2\")\n",
    "plt.title(\"Top 10 Nouns in context of 'Year Old'\",fontsize=30)\n",
    "plt.xticks(size=20,rotation=80)\n",
    "plt.yticks([])\n",
    "plt.subplot(1,2,2)\n",
    "pd.Series(verb).value_counts().head(10).plot(kind=\"bar\",figsize=(20,5),colormap=\"Set2\")\n",
    "plt.title(\"Top 10 Verbs in context of 'Year Old'\",fontsize=30)\n",
    "plt.xticks(size=20,rotation=80)\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['headline_text'].loc[index].tolist()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_s=data['headline_text'].str.match(r'(?=.*\\bcommits\\b)(?=.*\\bsuicide\\b).*$')\n",
    "text_s=data['headline_text'].loc[index].tolist()\n",
    "noun_s=[]\n",
    "for doc in nlp.pipe(text_s,n_threads=16,batch_size=1000):\n",
    "    try:\n",
    "        for c in doc:\n",
    "            if c.pos_=='NOUN':\n",
    "                noun_s.append(c.text)\n",
    "    except:\n",
    "        for c in doc:\n",
    "            noun_s.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(noun_s).value_counts().head(20).plot(\"bar\",figsize=(15,5),colormap=\"Set2\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of Nouns in the context of 'Commits Suicide'\",fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_s=data['headline_text'].str.match(r'(?=.*\\bcommits\\b)(?=.*\\bsuicide\\b).*$',case=False)\n",
    "index_farmer=data.loc[index_s]['headline_text'].str.match(r'farmer',case=False)\n",
    "index_stu=data.loc[index_s]['headline_text'].str.match(r'student',case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Approximately {} percent of suicides reported were student related\".format(round(np.sum(index_stu)/np.sum(index_s),2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Approximately {} percent of suicides reported were farmer related\".format(round(np.sum(index_farmer)/np.sum(index_s),2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_farmer=data['headline_text'].str.match(r'farmer|farmers',case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_f=data.loc[ind_farmer]['headline_text'].tolist()\n",
    "noun_f=[]\n",
    "verb_f=[]\n",
    "for doc in nlp.pipe(text_f,n_threads=16,batch_size=1000):\n",
    "    try:\n",
    "        for c in doc:\n",
    "            if c.pos_=='NOUN':\n",
    "                noun_f.append(c.text)\n",
    "            elif c.pos_==\"VERB\":\n",
    "                verb_f.append(c.text)\n",
    "    except:\n",
    "        for c in doc:\n",
    "            noun_f.append(\"\") \n",
    "            verb_f.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "pd.Series(noun_f).value_counts()[2:].head(10).plot(kind=\"bar\",figsize=(20,5),colormap=\"Set2\")\n",
    "plt.title(\"Top 10 Nouns in the context of 'Farmer(s)'\",fontsize=25)\n",
    "plt.xticks(size=20,rotation=80)\n",
    "plt.yticks([])\n",
    "plt.subplot(1,2,2)\n",
    "pd.Series(verb_f).value_counts().head(10).plot(kind=\"bar\",figsize=(20,5),colormap=\"Set2\")\n",
    "plt.title(\"Top 10 Verbs in the context of 'Farmer(s)'\",fontsize=25)\n",
    "plt.xticks(size=20,rotation=80)\n",
    "plt.yticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
